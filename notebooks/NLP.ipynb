{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ignacio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ignacio/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('../data/comments.csv', index_col='Id', skip_blank_lines=True)\n",
    "    df = df.dropna()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    exit()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text, flags=re.IGNORECASE)\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['Processed_Comment'] = df['Comment'].apply(preprocess_text)\n",
    "\n",
    "#TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000) \n",
    "X = vectorizer.fit_transform(df['Comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e390878aa0444934b18501262caa2b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing comments:   0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "def get_sentiment_batch(texts):\n",
    "    try:\n",
    "        results = sentiment_pipeline(texts, truncation=True, max_length=512, batch_size=32, device=0)\n",
    "        return [result['label'] for result in results]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in sentiment analysis: {e}\")\n",
    "        return [\"NEUTRAL\"] * len(texts)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize an empty list to store all sentiments\n",
    "all_sentiments = []\n",
    "\n",
    "# Create a tqdm progress bar for the entire dataset\n",
    "with tqdm(total=len(df), desc=\"Processing comments\") as pbar:\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        # Get the current batch\n",
    "        batch_texts = df['Processed_Comment'].iloc[i:i+batch_size].tolist()\n",
    "        \n",
    "        # Process the batch\n",
    "        batch_sentiments = get_sentiment_batch(batch_texts)\n",
    "        \n",
    "        # Extend the list of all sentiments\n",
    "        all_sentiments.extend(batch_sentiments)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        pbar.update(len(batch_texts))\n",
    "\n",
    "# Assign sentiments to DataFrame\n",
    "df['Sentiment'] = all_sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)\n",
    "plt.title('Distribution of Sentiments')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "df.to_csv('../data/comments_with_sentiment.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000019"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
